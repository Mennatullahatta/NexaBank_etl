# NexaBank ETL Pipeline

This project implements a scalable, modular ETL pipeline designed for banking and transactional data. It uses a multithreaded producer-consumer pattern to monitor new data files, validate them, transform the data, and load it efficiently into a data lake in Parquet format and HDFS.

## ğŸ“š Table of Contents

* [ğŸš€ Key Features](#-key-features)
* [ğŸ§  Architecture Overview](#-architecture-overview)
* [ğŸ“ Project Structure](#-project-structure)
* [ğŸ› ï¸ How It Works](#ï¸-how-it-works)
* [ğŸ“¦ Supported Input Formats](#-supported-input-formats)
* [ğŸ“„ Logging & Notifications](#-logging--notifications)
* [ğŸ“¬ Email Alerts](#-email-alerts)
* [ğŸ“‚ HDFS Integration](#-hdfs-integration)
* [âœ… Run the ETL System](#-run-the-etl-system)
* [ğŸ‘¥ Contributors](#-contributors)

## ğŸš€ Key Features

* ğŸ”„ Producer-Consumer Design Pattern using Python Queue and multithreading
* ğŸ“‚ File Monitor thread watches for incoming files and enqueues them
* ğŸ—ï¸ Pipeline thread dequeues files and:

  * Extracts raw data
  * Validates against a defined schema
  * Filters already-processed data using a state\_store
  * Transforms data using appropriate transformer modules
  * Loads data into Parquet + HDFS (via subprocess)
* âœ… Schema validation and stateful deduplication
* ğŸ“¬ Failure alerts via email
* ğŸ“œ Full logging system using a custom Logger

## ğŸ§  Architecture Overview

File Monitor Thread â†’ puts file path â†’ Shared Queue â†’ picked up by Pipeline Thread
Pipeline steps:

1. Extract
2. Validate
3. Check state\_store
4. Transform
5. Save as Parquet
6. Upload to HDFS
7. Log + Send email on failure


## ğŸ“ Project Structure


```markdown
## ğŸ“ Project Structure

```

src/
â”œâ”€â”€ main.py                      # Entry point of the system: creates a Pipeline instance, passes it to FileMonitor, and starts the monitoring
â”œâ”€â”€ file\_monitor/                # Contains FileMonitor: monitors folders, queues files, and drives the producer-consumer pattern
â”‚   â””â”€â”€ file\_monitor.py          # Starts two threads: one detects new files and enqueues them, the other pulls from the queue and passes files to the pipeline

â”œâ”€â”€ pipeline/                    # Contains all ETL logic (extraction, validation, transformation, loading, and error handling)
â”‚   â”œâ”€â”€ pipeline.py              # Core controller: orchestrates the ETL stages (extract, validate, filter via state store, transform, load)
â”‚                                # - Selects extractor by file extension (.csv, .json, .txt)
â”‚                                # - Validates data using schema
â”‚                                # - Transforms using dataset-specific logic
â”‚                                # - Writes to Parquet
â”‚                                # - Loads into HDFS
â”‚                                # - Logs status and notifies on failure

â”‚   â”œâ”€â”€ extractors/              # Contains Extractor classes for different file types:
â”‚   â”‚   â”œâ”€â”€ csv\_extractor.py     # Reads CSV files
â”‚   â”‚   â”œâ”€â”€ json\_extractor.py    # Reads JSON files
â”‚   â”‚   â””â”€â”€ txt\_extractor.py     # Reads delimited TXT files

â”‚   â”œâ”€â”€ validators/              # SchemaValidator ensures that input data adheres to expected schema (based on JSON definitions)

â”‚   â”œâ”€â”€ transformers/            # Dataset-specific transformation logic
â”‚   â”‚   â”œâ”€â”€ customer\_transformers.py
â”‚   â”‚   â”œâ”€â”€ credit\_transformers.py
â”‚   â”‚   â”œâ”€â”€ loans\_transformers.py
â”‚   â”‚   â”œâ”€â”€ money\_transfers\_transformers.py
â”‚   â”‚   â””â”€â”€ support\_transformers.py

â”‚   â”œâ”€â”€ loaders/                 # Responsible for writing output data
â”‚   â”‚   â”œâ”€â”€ parquet\_loader.py    # Saves cleaned DataFrame to local Parquet file (./tmp)
â”‚   â”‚   â””â”€â”€ hdfs\_loader.py       # Uploads Parquet file to HDFS using subprocess (Hive-compatible staging directory)

â”‚   â”œâ”€â”€ logger/                  # Custom logger class that writes detailed logs to ./logs/etl.log

â”‚   â”œâ”€â”€ notifier/                # Sends email notifications when the pipeline fails
â”‚   â”‚   â””â”€â”€ email\_notifier.py    # Uses SMTP (Gmail-based) to notify stakeholders

â”‚   â”œâ”€â”€ state\_store/             # Tracks previously processed records to prevent duplicate processing
â”‚   â”‚   â””â”€â”€ state.py             # Reads/writes per-file state and filters already-processed rows

â”‚   â””â”€â”€ support/                 # Contains schema definitions and helper files
â”‚       â”œâ”€â”€ schemas.json         # JSON Schema definitions used by validators
â”‚       â””â”€â”€ english\_words.txt    # A wordlist used for brute-force decryption in the loans transformation

```
```





## ğŸ› ï¸ How It Works

The system consists of two main threads:

**1. File Monitor (Producer)**

* Monitors a directory for new files
* Pushes file paths into a shared queue

**2. Pipeline (Consumer)**

* Dequeues a file path
* Extracts data using the appropriate extractor
* Validates the data structure using a schema
* Checks state\_store to avoid reprocessing
* Applies dataset-specific transformations
* Saves final data as a Parquet file
* Uploads the file to HDFS using subprocess
* Logs each step
* Sends email alert if an error occurs

## ğŸ“¦ Supported Input Formats

* CSV
* JSON
* TXT

You can extend the supported formats by adding extractors in `src/pipeline/extractors/`.

## ğŸ“„ Logging & Notifications

* Logs are written to `logs/etl.log`
* On failure:

  * Logs include full error details
  * Email alert is sent to the configured recipients (set in `notifier/email_notifier.py`)


## ğŸ“¬ Email Alerts

If the pipeline encounters any error:

* The exception is logged
* An email is automatically sent to the configured address with error details

## ğŸ“‚ HDFS Integration

- Final DataFrames are first saved locally as `.parquet` using a **ParquetLoader** class, typically into a temporary directory.
- The **HdfsLoader** class then moves these files into HDFS using a `subprocess` call with the `hdfs dfs -put` command.
- The target HDFS directory is configured to align with **Hive external tables**, enabling direct querying from Hive.
- Ensure Hadoop is installed and properly configured in the environment for successful upload and Hive integration.

  ## âœ… Run the ETL System

1. **Start the services using Docker Compose**

   ```bash
   docker-compose up -d
   ```

2. **Access the `etl_py` container**

   ```bash
   docker exec -it etl_py bash
   ```

3. **Run the ETL process**

   ```bash
   cd src/
   python main.py
   ```


## ğŸ‘¥ Contributors

- **Ahmed Otifi** ğŸ”— [https://github.com/otifi3](https://github.com/otifi3)  
- **Hania Hesham** ğŸ”— [https://github.com/HaniaHesham99](https://github.com/HaniaHesham99)  
- **Mennatullah** ğŸ”— [https://github.com/Mennatullahatta](https://github.com/Mennatullahatta)

